{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient search\n",
    "In this live script we solve the following simple problem\n",
    "\n",
    "$$\\min\\sum_{k=0}^{1} x_{k}^{2}+u_{k}^{2}+g_{2}\\left(x_{2}\\right)$$\n",
    "\n",
    "$$ \\text{s.t. } x_{k+1}=x_{k}+u_{k} \\quad k \\in\\{0,1\\}$$\n",
    "\n",
    "with\n",
    "\n",
    "$$g_{2}\\left(x_{2}\\right)=x_{2}^2$$\n",
    "\n",
    "and initial condition $x_0=1$, using a gradient decend method. In fact, we \n",
    "can convert the problem of finding an optimal path for this problem into the \n",
    "following optimization problem (by simply writting $x_1$, $x_2$ as a function \n",
    "of $u_0, u_1$). \n",
    "\n",
    "$$\\min_{u_0,u_1} x_{0}^{2}+u_{0}^{2}+x_{1}^{2}+u_{1}^{2}+e^{x_{2}}=x_{0}^{2}+u_{0}^{2}+\\left(x_{0}+u_{0}\\right)^{2}+u_{1}^{2}+e^{\\left(x_{0}+u_{0}+u_{1}\\right)}$$\n",
    "\n",
    "This gradient desend method uses (an exhaustive) line search and is described \n",
    "by the following equation\n",
    "\n",
    "$$ u^{(k+1)} =u^{(k)}-\\alpha_{k} \\frac{\\partial J}{\\partial u}\\left(u^{(k)}\\right) \n",
    "$$\n",
    "\n",
    "$$ \\alpha_{k} =arg \\min _{\\beta \\in[0, \\infty)} J\\left(u^{(k)}-\\beta d\\right), \n",
    "\\quad d=\\frac{\\partial J}{\\partial u}\\left(u^{(k)}\\right) $$\n",
    "\n",
    "$$\\\\ u^{(k)} =\\left[u_{0}^{(k)} u_{1}^{(k)}\\right]^T $$\n",
    "\n",
    "The function is plotted next."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib ipympl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradJ(x):\n",
    "    grad = np.array([[2*x[0][0]+2*(1+x[0][0])+np.exp(1+x[0][0]+x[1][0])],[2*x[1][0]+np.exp(1+x[0][0]+x[1][0])]])\n",
    "    return grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def functionJ(x):\n",
    "    cost = 1+x[0][0]**2+(1+x[0][0])**2+x[1][0]**2+np.exp(1+x[0][0]+x[1][0])\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradmethlinesearch(J,dJ,x0,epsilon,L,niter):\n",
    "\n",
    "    n      = x0.shape[0]\n",
    "    X      = np.zeros((n,niter+1))\n",
    "    X[:,[0]] = x0\n",
    "\n",
    "    for k in range(niter):\n",
    "        d = dJ(X[:,[k]])\n",
    "        \n",
    "        vectbeta = np.arange(epsilon,L,epsilon)\n",
    "        c = np.zeros(vectbeta.shape[0])\n",
    "        for i in range(vectbeta.shape[0]):\n",
    "            c[i] = J(X[:,[k]]-vectbeta[i]*d)\n",
    "        \n",
    "        indmin = np.argmin(c)\n",
    "        X[:,[k+1]] = X[:,[k]]-vectbeta[indmin]*d\n",
    "    \n",
    "    xsol = X[-1]\n",
    "    return X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "The function `gradmethlinesearch` runs the gradient search given handlers \n",
    "for the function and function derivative, the initial guess, and three parameters. \n",
    "The first two parameters, `epsilon` and `L` determine the values of $\\beta$ \n",
    "to be selected in the exhaustive search in the selection of $\\alpha_k$; these \n",
    "are evenly spaced values between $0$ and $L$ and spaced by $\\epsilon$. The third \n",
    "parameter is the number of iterations (steps or evaluations of gradient) of \n",
    "the algorithm. The output is a matrix with $L$ columns where each column contains \n",
    "the guess of the optimal solution at step $k$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "u0 = np.arange(-3,1,0.001)\n",
    "u1 = np.arange(-3,2,0.001)\n",
    "X,Y = np.meshgrid(u0,u1)\n",
    "Z = 1+X**2+(1+X)**2+Y**2+np.exp(1+X+Y)\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "ax.plot_surface(X,Y,Z)\n",
    "plt.xlabel('u0')\n",
    "plt.ylabel('u1');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x0      = np.array([[1],[1]])\n",
    "epsilon = 0.001 # epsilon and L are parameters for the exhaustive line search\n",
    "L       = 3     \n",
    "niter   = 5 # number of iterations (steps, or evaluations of gradien) of the gradient method\n",
    "Xit = gradmethlinesearch(functionJ ,gradJ, np.array([[1],[1]]), 0.001, 3, 5)\n",
    "Xit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
